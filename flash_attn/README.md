# Attention
* Standard Attention
  
  <img width="894" height="168" alt="Screenshot from 2025-09-14 10-58-05" src="https://github.com/user-attachments/assets/dfc48d67-b8c2-4d6f-ac53-4383940b9109" />
  
* Flash Attention
  <img width="1207" height="753" alt="image" src="https://github.com/user-attachments/assets/1fa7b3fe-c10a-4b85-9d4c-1eba9d269f0f" />

  There is a mistake in the above algorithm in line 10, inverse shouldn't be there... see [Algo Typo Issue](https://github.com/Dao-AILab/flash-attention/issues/991)
  
  <img width="956" height="497" alt="image" src="https://github.com/user-attachments/assets/672ae0e1-0c4d-43ce-a30e-6d6640e4f041" />
  
  <img width="1510" height="802" alt="image" src="https://github.com/user-attachments/assets/a9d226e4-c7d7-4641-a721-ecd772ad25f7" />
  
  <img width="1024" height="529" alt="image" src="https://github.com/user-attachments/assets/fd57d005-895c-4614-8816-f22565cd13c5" />
  
  <img width="1508" height="453" alt="image" src="https://github.com/user-attachments/assets/d72b21e0-dfc6-4e36-b005-52b06b7db7cc" />
